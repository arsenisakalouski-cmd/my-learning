# –î–µ–Ω—å 25: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

## üéØ –ß—Ç–æ –∏–∑—É—á–∏–ª–∏:

### 1. –î–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π (Decision Trees)
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç (–±–ª–æ–∫-—Å—Ö–µ–º–∞ –≤–æ–ø—Ä–æ—Å–æ–≤)
- DecisionTreeRegressor / DecisionTreeClassifier
- –ü–∞—Ä–∞–º–µ—Ç—Ä max_depth (–≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞)
- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–µ—Ä–µ–≤–∞
- –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: –ø–æ–Ω—è—Ç–Ω–æ, —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å—é
- –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏: –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ, –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å

### 2. Random Forest (–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å)
- –ú–Ω–æ–≥–æ –¥–µ—Ä–µ–≤—å–µ–≤ –≤–º–µ—Å—Ç–µ = —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ
- –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: n_estimators, max_depth
- Feature importance (–≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
- –õ—É—á—à–µ —á–µ–º –æ–¥–Ω–æ –¥–µ—Ä–µ–≤–æ

### 3. Feature Engineering (–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
- –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
- –ö–æ–º–±–∏–Ω–∞—Ü–∏–∏: –¥–µ–ª–µ–Ω–∏–µ, —É–º–Ω–æ–∂–µ–Ω–∏–µ
- –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: –≤–æ–∑—Ä–∞—Å—Ç –∏–∑ –≥–æ–¥–∞
- –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ —á–∏—Å–µ–ª: –≤—ã—Å–æ–∫–∏–π/–Ω–∏–∑–∫–∏–π
- One-Hot Encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
- –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ 10-30%

### 4. Cross-Validation (–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è)
- K-Fold —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
- –ú–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- –ë–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
- cross_val_score()
- –û–±—ã—á–Ω–æ cv=5 –∏–ª–∏ cv=10

### 5. –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç
- –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª ML –ø—Ä–æ–µ–∫—Ç–∞
- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ü–µ–Ω –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
- –û—Ç –¥–∞–Ω–Ω—ã—Ö –¥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è

## üìÅ –§–∞–π–ª—ã:
```
Day 25/
‚îú‚îÄ‚îÄ 01_decision_tree.py          # –î–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π
‚îú‚îÄ‚îÄ 02_random_forest_simple.py   # Random Forest
‚îú‚îÄ‚îÄ 03_feature_engineering.py    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
‚îú‚îÄ‚îÄ 04_cross_validation.py       # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
‚îú‚îÄ‚îÄ 05_final_project.py          # –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç
‚îî‚îÄ‚îÄ README.md                    # –≠—Ç–æ—Ç —Ñ–∞–π–ª
```

## üå≥ –î–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π:

### –ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç:
```
         –ü–ª–æ—â–∞–¥—å <= 70–º¬≤?
         /              \
      –î–ê                –ù–ï–¢
      /                  \
–î–µ—à—ë–≤–∞—è            –ö–æ–º–Ω–∞—Ç <= 2?
–∫–≤–∞—Ä—Ç–∏—Ä–∞           /          \
                 –î–ê           –ù–ï–¢
                 /             \
          –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞    –î–æ—Ä–æ–≥–∞—è
```

### –ö–æ–¥:
```python
from sklearn.tree import DecisionTreeRegressor

tree = DecisionTreeRegressor(
    max_depth=5,      # ‚Üê –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ (3-10)
    random_state=42
)

tree.fit(X_train, y_train)
predictions = tree.predict(X_test)
```

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
- **max_depth** - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ (–æ–±—ã—á–Ω–æ 3-10)
- **min_samples_split** - –º–∏–Ω–∏–º—É–º –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
- **min_samples_leaf** - –º–∏–Ω–∏–º—É–º –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –ª–∏—Å—Ç–µ

## üå≤ Random Forest:

### –ò–¥–µ—è:
```
1 –¥–µ—Ä–µ–≤–æ = –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ
100 –¥–µ—Ä–µ–≤—å–µ–≤ = —Å—Ç–∞–±–∏–ª—å–Ω–æ (—É—Å—Ä–µ–¥–Ω—è–µ–º)

–ö–∞–∫ —Å–ø—Ä–æ—Å–∏—Ç—å —Å–æ–≤–µ—Ç:
‚ùå –£ 1 —á–µ–ª–æ–≤–µ–∫–∞ - –º–æ–∂–µ—Ç –æ—à–∏–±–∏—Ç—å—Å—è
‚úì –£ 100 —á–µ–ª–æ–≤–µ–∫ - –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ø—Ä–∞–≤—ã
```

### –ö–æ–¥:
```python
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(
    n_estimators=100,  # ‚Üê –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤
    max_depth=5,       # ‚Üê –≥–ª—É–±–∏–Ω–∞ –∫–∞–∂–¥–æ–≥–æ
    n_jobs=-1,         # ‚Üê –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã
    random_state=42
)

rf.fit(X_train, y_train)
predictions = rf.predict(X_test)

# –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
importances = rf.feature_importances_
```

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
- **n_estimators** - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ (100-500)
- **max_depth** - –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤ (5-10)
- **n_jobs=-1** - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è

## üîß Feature Engineering:

### –¢–∏–ø—ã –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:

**1. –ö–æ–º–±–∏–Ω–∞—Ü–∏–∏:**
```python
df['–ü–ª–æ—â–∞–¥—å_–Ω–∞_–∫–æ–º–Ω–∞—Ç—É'] = df['–ü–ª–æ—â–∞–¥—å'] / df['–ö–æ–º–Ω–∞—Ç—ã']
df['–ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ'] = df['–ü—Ä–∏–∑–Ω–∞–∫1'] * df['–ü—Ä–∏–∑–Ω–∞–∫2']
```

**2. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è:**
```python
df['–í–æ–∑—Ä–∞—Å—Ç'] = 2024 - df['–ì–æ–¥_—Ä–æ–∂–¥–µ–Ω–∏—è']
df['–õ–æ–≥–∞—Ä–∏—Ñ–º'] = np.log(df['–ó–Ω–∞—á–µ–Ω–∏–µ'] + 1)
```

**3. –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ —á–∏—Å–µ–ª:**
```python
df['–í—ã—Å–æ–∫–∏–π'] = (df['–†–æ—Å—Ç'] > 180).astype(int)
df['–ì—Ä—É–ø–ø–∞'] = pd.cut(df['–í–æ–∑—Ä–∞—Å—Ç'], bins=[0, 18, 65, 100], 
                       labels=['–†–µ–±—ë–Ω–æ–∫', '–í–∑—Ä–æ—Å–ª—ã–π', '–ü–µ–Ω—Å–∏–æ–Ω–µ—Ä'])
```

**4. One-Hot Encoding:**
```python
df_encoded = pd.get_dummies(df, columns=['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'])

# –ë–´–õ–û:
# –¶–≤–µ—Ç
# –ö—Ä–∞—Å–Ω—ã–π
# –°–∏–Ω–∏–π

# –°–¢–ê–õ–û:
# –¶–≤–µ—Ç_–ö—Ä–∞—Å–Ω—ã–π  –¶–≤–µ—Ç_–°–∏–Ω–∏–π
# 1             0
# 0             1
```

## ‚úÖ Cross-Validation:

### K-Fold —Å—Ö–µ–º–∞:
```
Fold 1: [TEST][train][train][train][train]
Fold 2: [train][TEST][train][train][train]
Fold 3: [train][train][TEST][train][train]
Fold 4: [train][train][train][TEST][train]
Fold 5: [train][train][train][train][TEST]

‚Üí 5 –æ—Ü–µ–Ω–æ–∫ ‚Üí —É—Å—Ä–µ–¥–Ω—è–µ–º
```

### –ö–æ–¥:
```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(
    model,           # –º–æ–¥–µ–ª—å
    X, y,            # –í–°–ï –¥–∞–Ω–Ω—ã–µ
    cv=5,            # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ fold'–æ–≤
    scoring='r2'     # –º–µ—Ç—Ä–∏–∫–∞
)

print(f"–°—Ä–µ–¥–Ω–µ–µ: {scores.mean():.3f}")
print(f"–†–∞–∑–±—Ä–æ—Å: {scores.std():.3f}")
```

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
- ‚úì –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö (<1000)
- ‚úì –ù—É–∂–Ω–∞ —Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
- ‚úì –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:

| –ú–æ–¥–µ–ª—å | –ü–ª—é—Å—ã | –ú–∏–Ω—É—Å—ã | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |
|--------|-------|--------|-------------------|
| **Linear Regression** | –ë—ã—Å—Ç—Ä–∞—è, –ø—Ä–æ—Å—Ç–∞—è | –¢–æ–ª—å–∫–æ –ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ | –ü—Ä–æ—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ |
| **Decision Tree** | –ü–æ–Ω—è—Ç–Ω–∞—è, –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å | –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ | –ù—É–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è |
| **Random Forest** | –¢–æ—á–Ω–∞—è, —Å—Ç–∞–±–∏–ª—å–Ω–∞—è | –ú–µ–¥–ª–µ–Ω–Ω–∞—è, —Å–ª–æ–∂–Ω–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å | –ù—É–∂–Ω–∞ –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å |

## üéØ –ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å ML –ø—Ä–æ–µ–∫—Ç–∞:
```python
# 1. –î–ê–ù–ù–´–ï
df = pd.read_csv('data.csv')

# 2. –ò–ó–£–ß–ï–ù–ò–ï
df.describe()
df.plot()

# 3. FEATURE ENGINEERING
df['–Ω–æ–≤—ã–π_–ø—Ä–∏–∑–Ω–∞–∫'] = df['–ø—Ä–∏–∑–Ω–∞–∫1'] / df['–ø—Ä–∏–∑–Ω–∞–∫2']
df = pd.get_dummies(df, columns=['–∫–∞—Ç–µ–≥–æ—Ä–∏—è'])

# 4. –ü–û–î–ì–û–¢–û–í–ö–ê
X = df.drop('—Ü–µ–ª—å', axis=1).values
y = df['—Ü–µ–ª—å'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 5. –û–ë–£–ß–ï–ù–ò–ï
model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, y_train)

# 6. –û–¶–ï–ù–ö–ê
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)

# 7. CROSS-VALIDATION
scores = cross_val_score(model, X, y, cv=5)

# 8. –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í
importances = model.feature_importances_

# 9. –ü–†–ò–ú–ï–ù–ï–ù–ò–ï
predictions = model.predict(–Ω–æ–≤—ã–µ_–¥–∞–Ω–Ω—ã–µ)
```

## üí° –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã:

### Feature Engineering:
1. **–ù–∞—á–Ω–∏—Ç–µ —Å –ø—Ä–æ—Å—Ç–æ–≥–æ** - –¥–µ–ª–µ–Ω–∏–µ, —É–º–Ω–æ–∂–µ–Ω–∏–µ
2. **–ü–æ–¥—É–º–∞–π—Ç–µ –æ –∑–∞–¥–∞—á–µ** - —á—Ç–æ –≤–∞–∂–Ω–æ?
3. **–ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –≤–∞–∂–Ω–æ—Å—Ç—å** - feature_importances_
4. **–£–¥–∞–ª—è–π—Ç–µ –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–µ** - —É–ø—Ä–æ—â–∞–π—Ç–µ –º–æ–¥–µ–ª—å

### –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏:
1. **–ù–∞—á–Ω–∏—Ç–µ —Å –ø—Ä–æ—Å—Ç–æ–π** - Linear Regression
2. **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥–µ—Ä–µ–≤–æ** - Decision Tree
3. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–µ—Å** - Random Forest
4. **–°—Ä–∞–≤–Ω–∏—Ç–µ —á–µ—Ä–µ–∑ CV** - –≤—ã–±–µ—Ä–∏—Ç–µ –ª—É—á—à—É—é

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
- **max_depth**: –Ω–∞—á–Ω–∏—Ç–µ —Å 5, —É–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
- **n_estimators**: 100-200 –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
- **cv**: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ 5 –¥–ª—è –Ω–∞—á–∞–ª–∞

## üîç –ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏:

‚ùå **–ù–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏** - —É–ø—É—Å–∫–∞–µ—Ç–µ —É–ª—É—á—à–µ–Ω–∏–µ  
‚ùå **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –¥–µ—Ä–µ–≤–∞** - max_depth —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π  
‚ùå **–ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CV** - –Ω–µ—Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞  
‚ùå **–ó–∞–±—ã—Ç—å One-Hot –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π** - –º–æ–¥–µ–ª—å –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç —Ç–µ–∫—Å—Ç  
‚ùå **–†–∞–∑–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ train/test** - –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏  

## üìà –ú–µ—Ç—Ä–∏–∫–∏:

### –†–µ–≥—Ä–µ—Å—Å–∏—è:
```python
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

r2 = r2_score(y_test, y_pred)              # 0-1, –ª—É—á—à–µ‚Üí1
mae = mean_absolute_error(y_test, y_pred)  # –º–µ–Ω—å—à–µ‚Üí–ª—É—á—à–µ
rmse = np.sqrt(mean_squared_error(...))    # –º–µ–Ω—å—à–µ‚Üí–ª—É—á—à–µ
```

### –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:
```python
from sklearn.metrics import accuracy_score, confusion_matrix

acc = accuracy_score(y_test, y_pred)  # 0-100%
```

## üéì –ß—Ç–æ –¥–∞–ª—å—à–µ:

### –°–ª–µ–¥—É—é—â–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏:
1. **Gradient Boosting** - –µ—â—ë —Ç–æ—á–Ω–µ–µ RF
2. **Hyperparameter Tuning** - GridSearch, RandomSearch
3. **Feature Selection** - –≤—ã–±–æ—Ä –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
4. **Scaling** - –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
5. **Pipeline** - –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞

### –î—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏:
- XGBoost, LightGBM - —Ç–æ–ø–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
- Support Vector Machines
- K-Nearest Neighbors
- Neural Networks

## üèÜ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞:

**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ü–µ–Ω –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π

**–î–∞–Ω–Ω—ã–µ:** 1000 –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π, 6 –∏—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

**Feature Engineering:** +4 –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞

**–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å:** Random Forest
- R¬≤ = ~0.95
- MAE = ~50 —Ç—ã—Å. —Ä—É–±

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –û—Ü–µ–Ω–∫–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ª—é–±–æ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è

## ‚úÖ –ß–µ–∫-–ª–∏—Å—Ç –æ—Å–≤–æ–µ–Ω–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤:

- [x] –ü–æ–Ω–∏–º–∞—é –∫–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç –¥–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π
- [x] –£–º–µ—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Random Forest
- [x] –ú–æ–≥—É —Å–æ–∑–¥–∞–≤–∞—Ç—å –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
- [x] –ó–Ω–∞—é –∫–∞–∫ –¥–µ–ª–∞—Ç—å One-Hot Encoding
- [x] –£–º–µ—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Cross-Validation
- [x] –ú–æ–≥—É —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –º–æ–¥–µ–ª–∏
- [x] –ü–æ–Ω–∏–º–∞—é feature importance
- [x] –ú–æ–≥—É —Å–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω—ã–π ML –ø—Ä–æ–µ–∫—Ç

## üéØ –°–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å:

**–î–µ–Ω—å 26** - –£–ª—É—á—à–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:
- Hyperparameter tuning
- GridSearch
- Feature selection
- –†–∞–±–æ—Ç–∞ —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏

---

**–ü–æ–∑–¥—Ä–∞–≤–ª—è—é! –û—Å–≤–æ–µ–Ω—ã –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ ML! üéâ**

**Random Forest - –æ–¥–Ω–∞ –∏–∑ —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏!**