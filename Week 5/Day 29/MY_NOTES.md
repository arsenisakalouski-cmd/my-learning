### День 29 (файл 01): Перцептрон ⭐⭐⭐

**Главная идея:** Перцептрон = простейшая нейросеть (1 нейрон)

**Структура:**
```
Входы (X) → Веса (W) → Сумма (Σ) → Активация → Выход (y)
```

**Формула:**
```python
z = X1*W1 + X2*W2 + ... + Xn*Wn + bias
y = activation(z)
```

**Обучение (правило перцептрона):**
```python
W_new = W_old + learning_rate * error * X
error = y_true - y_pred
```

**Что может:**
- Линейно разделимые задачи (AND, OR)

**Что НЕ может:**
- Нелинейные задачи (XOR)

**Решение:** Многослойные сети!




### День 29 (файл 02): MLP - Многослойная сеть ⭐⭐⭐

**Простыми словами:** 
MLP = несколько перцептронов в слоях
Первый слой → обрабатывает данные → второй слой → результат

**Зачем нужно:**
Перцептрон (1 слой) - только линейные границы
MLP (2+ слоя) - ЛЮБЫЕ границы (даже XOR!)

**Архитектура:**
```
Вход (2) → Скрытый слой (4) → Выход (1)
   X1 ──┐      H1 ──┐
        ├─────┤      ├──> Y
   X2 ──┘      H2 ──┘
```

**Обучение (простое объяснение):**

1. **Forward (вперёд):**
   - Данные идут через слои
   - Каждый нейрон: взвешенная сумма + активация
   
2. **Считаем ошибку:**
   - Сравниваем выход с правильным ответом
   
3. **Backward (назад):**
   - Распространяем ошибку обратно
   - Обновляем веса (делаем их лучше)

**Функции активации (зачем):**
Без активации сеть = линейная (бесполезна!)
С активацией = нелинейная (мощная!)

- **Sigmoid:** для вероятностей (0-1)
- **ReLU:** самая популярная, быстрая
- **Tanh:** как sigmoid, но (-1, 1)

**Код (без математики):**
```python
class MLP:
    def forward(self, X):
        # Слой 1: вход → скрытый
        hidden = sigmoid(X @ W1 + b1)
        
        # Слой 2: скрытый → выход
        output = sigmoid(hidden @ W2 + b2)
        
        return output
    
    def backward(self, X, y, output):
        # Считаем насколько ошиблись
        error = output - y
        
        # Обновляем веса (упрощённо)
        W2 -= learning_rate * error * hidden
        W1 -= learning_rate * error_hidden * X
```

**Главное:**
- MLP решает то, что перцептрон не может (XOR)
- Обучение = подбор весов через примеры
- Больше слоёв = сложнее задачи


### День 29 (файл 03): Keras - простое создание сетей ⭐⭐⭐

**Что это:** Keras = конструктор LEGO для нейросетей

**Вместо 100 строк кода теперь 3 строки!**

**Три шага:**
```python
# 1. СОЗДАТЬ (как слои лего)
model = Sequential([
    Dense(4, activation='sigmoid'),  # скрытый слой
    Dense(1, activation='sigmoid')   # выходной слой
])

# 2. НАСТРОИТЬ (как готовить)
model.compile(
    optimizer='adam',              # как обучаться
    loss='binary_crossentropy',    # что минимизировать
    metrics=['accuracy']           # что смотреть
)

# 3. ОБУЧИТЬ (нажать "старт")
model.fit(X, y, epochs=500)
```

**Что означают слова:**

- **Sequential** = слои идут друг за другом (вход→скрытый→выход)
- **Dense** = полносвязный слой (все соединены со всеми)
- **activation** = функция которая решает передавать сигнал или нет
- **optimizer** = как умнеть (adam - самый популярный, умный)
- **loss** = насколько плохо (чем меньше - тем лучше)
- **epochs** = сколько раз учиться на данных

**Зачем что:**

- `sigmoid` - сжимает в 0-1 (для вероятностей)
- `adam` - умный способ обучения (лучше чем просто gradient descent)
- `binary_crossentropy` - для задач "да/нет"

**Сохранить модель:**
```python
model.save('my_model.h5')
model = keras.models.load_model('my_model.h5')
```

### День 29 (файл 04): Практический пример ⭐⭐⭐

**Задача:** Предсказать купит ли клиент товар

**Полный процесс (7 шагов):**
```python
# 1. ДАННЫЕ
X = [[возраст, доход, время], ...]
y = [0, 1, 1, 0, ...]  # купил или нет

# 2. МАСШТАБИРОВАНИЕ (ВАЖНО!)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 3. СОЗДАТЬ МОДЕЛЬ
model = Sequential([
    Dense(8, activation='relu', input_shape=(3,)),  # скрытый
    Dense(4, activation='relu'),                     # скрытый
    Dense(1, activation='sigmoid')                   # выход
])

# 4. НАСТРОИТЬ
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# 5. ОБУЧИТЬ
history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2
)

# 6. ОЦЕНИТЬ
test_acc = model.evaluate(X_test, y_test)[1]
print(f"Точность: {test_acc:.2%}")

# 7. ПРЕДСКАЗАТЬ
new_data = scaler.transform([[25, 80, 30]])  # новый клиент
prediction = model.predict(new_data)
print(f"Вероятность покупки: {prediction[0][0]:.1%}")
```

**Важные моменты:**

1. **Масштабирование обязательно!**
   - fit_transform на train
   - transform на test/new

2. **validation_split=0.2**
   - 20% train → для проверки
   - Видим переобучение

3. **batch_size=32**
   - Обновляет веса после каждых 32 примеров
   - Баланс скорости и стабильности

4. **Графики:**
   - Train vs Validation близки → хорошо
   - Train падает, Validation растёт → переобучение

**Типичные ошибки:**

❌ Забыть масштабирование → плохое качество
❌ Использовать fit_transform на test → утечка данных
❌ Слишком много эпох → переобучение